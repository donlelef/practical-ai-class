{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqvOVtwQObvR"
   },
   "source": [
    "# In-Depth: Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziym0pA4ObvR"
   },
   "source": [
    "> This notebook was adapted from [PythonDataScienceHandbook's GitHub](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.08-Random-Forests.ipynb). Have a look to the repository: it contains great examples for many models.\n",
    "\n",
    "\n",
    "We'll take a look at another powerful algorithm—a non-parametric algorithm called *random forests*.\n",
    "Random forests are an example of an *ensemble* method, meaning that it relies on aggregating the results of an ensemble of simpler estimators.\n",
    "The somewhat surprising result with such ensemble methods is that the sum can be greater than the parts: that is, a majority vote among a number of estimators can end up being better than any of the individual estimators doing the voting!\n",
    "We will see examples of this in the following sections.\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1663246461955,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "Jj2NvaKsObvS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj0KWFKnObvT"
   },
   "source": [
    "## Motivating Random Forests: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59F0p2_VObvU"
   },
   "source": [
    "Random forests are an example of an *ensemble learner* built on decision trees.\n",
    "For this reason we'll start by discussing decision trees themselves.\n",
    "\n",
    "Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification.\n",
    "For example, if you wanted to build a decision tree to classify an animal you come across while on a hike, you might construct the one shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThXZn4uQObvV"
   },
   "source": [
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.08-decision-tree.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFZgIwxzObvV"
   },
   "source": [
    "The binary splitting makes this extremely efficient: in a well-constructed tree, each question will cut the number of options by approximately half, very quickly narrowing the options even among a large number of classes.\n",
    "The trick, of course, comes in deciding which questions to ask at each step.\n",
    "In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data: that is, each node in the tree splits the data into two groups using a cutoff value within one of the features.\n",
    "Let's now look at an example of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-5sZ9wAObvW"
   },
   "source": [
    "### Creating a decision tree\n",
    "\n",
    "Consider the following two-dimensional data, which has one of four class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "executionInfo": {
     "elapsed": 1148,
     "status": "ok",
     "timestamp": 1663246463099,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "-0xz6W2pObvX",
    "outputId": "35a125cd-6fbf-4340-f8de-b5443f929352"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\"rainbow\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp1Wg7isObvY"
   },
   "source": [
    "A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it.\n",
    "This figure presents a visualization of the first four levels of a decision tree classifier for this data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_4yCE6UObvY"
   },
   "source": [
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.08-decision-tree-levels.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywYyrIJkObvZ"
   },
   "source": [
    "Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch.\n",
    "Except for nodes that contain all of one color, at each level *every* region is again split along one of the two features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnOqLv51ObvZ"
   },
   "source": [
    "This process of fitting a decision tree to our data can be done in Scikit-Learn with the ``DecisionTreeClassifier`` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1663246463100,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "MiZTMJuGObvZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlNltto6Obva"
   },
   "source": [
    "Let's write a quick utility function to help us visualize the output of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1663246463100,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "Sp4OOUicObva"
   },
   "outputs": [],
   "source": [
    "def visualize_classifier(model, X, y, ax=None, cmap=\"rainbow\"):\n",
    "    ax = ax or plt.gca()\n",
    "\n",
    "    # Plot the training points\n",
    "    ax.scatter(\n",
    "        X[:, 0], X[:, 1], c=y, s=30, cmap=cmap, clim=(y.min(), y.max()), zorder=3\n",
    "    )\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # fit the estimator\n",
    "    model.fit(X, y)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200), np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    ax.contourf(\n",
    "        xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=cmap, zorder=1\n",
    "    )\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A48hA9ioObva"
   },
   "source": [
    "Now we can examine what the decision tree classification looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 1177,
     "status": "ok",
     "timestamp": 1663246464273,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "s_BzmmqeObva",
    "outputId": "cb59cf2c-90c1-4ae2-f4f9-8c1a11f5f455"
   },
   "outputs": [],
   "source": [
    "visualize_classifier(DecisionTreeClassifier(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZjkRbnhObvb"
   },
   "source": [
    "Notice that as the depth increases, we tend to get very strangely shaped classification regions; for example, at a depth of five, there is a tall and skinny purple region between the yellow and blue regions.\n",
    "It's clear that this is less a result of the true, intrinsic data distribution, and more a result of the particular sampling or noise properties of the data.\n",
    "That is, this decision tree, even at only five levels deep, is clearly over-fitting our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09Qfo3nLObvc"
   },
   "source": [
    "### Decision trees and over-fitting\n",
    "\n",
    "Such over-fitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from.\n",
    "Another way to see this over-fitting is to look at models trained on different subsets of the data—for example, in this figure we train two different trees, each on half of the original data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu93_iz2Obvc"
   },
   "source": [
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.08-decision-tree-overfitting.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K2-LiFAObvc"
   },
   "source": [
    "It is clear that in some places, the two trees produce consistent results (e.g., in the four corners), while in other places, the two trees give very different classifications (e.g., in the regions between any two clusters).\n",
    "The key observation is that the inconsistencies tend to happen where the classification is less certain, and thus by using information from *both* of these trees, we might come up with a better result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt_ay9FhObvd"
   },
   "source": [
    "Just as using information from two trees improves our results, we might expect that using information from many trees would improve our results even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4rokXnWObvd"
   },
   "source": [
    "## Ensembles of Estimators: Random Forests\n",
    "\n",
    "This notion—that multiple overfitting estimators can be combined to reduce the effect of this overfitting—is what underlies an ensemble method called *bagging*.\n",
    "Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which over-fits the data, and averages the results to find a better classification.\n",
    "An ensemble of randomized decision trees is known as a *random forest*.\n",
    "\n",
    "This type of bagging classification can be done manually using Scikit-Learn's ``BaggingClassifier`` meta-estimator, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1663246464274,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "Rvg0WLvsObvd",
    "outputId": "a0c6665f-fa96-4356-fdd4-3d13f418582f"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1)\n",
    "\n",
    "bag.fit(X, y)\n",
    "visualize_classifier(bag, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNFG560FObvd"
   },
   "source": [
    "In this example, we have randomized the data by fitting each estimator with a random subset of 80% of the training points.\n",
    "In practice, decision trees are more effectively randomized by injecting some stochasticity in how the splits are chosen: this way all the data contributes to the fit each time, but the results of the fit still have the desired randomness.\n",
    "For example, when determining which feature to split on, the randomized tree might select from among the top several features.\n",
    "You can read more technical details about these randomization strategies in the [Scikit-Learn documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest) and references within.\n",
    "\n",
    "In Scikit-Learn, such an optimized ensemble of randomized decision trees is implemented in the ``RandomForestClassifier`` estimator, which takes care of all the randomization automatically.\n",
    "All you need to do is select a number of estimators, and it will very quickly (in parallel, if desired) fit the ensemble of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1663246465075,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "LZdtrIbTObve",
    "outputId": "3c5ea128-ecf6-4071-8182-786f1a8de764"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "visualize_classifier(model, X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXJzxSfzObve"
   },
   "source": [
    "We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qw_w3-bObve"
   },
   "source": [
    "## Random Forest Regression\n",
    "\n",
    "In the previous section we considered random forests within the context of classification.\n",
    "Random forests can also be made to work in the case of regression (that is, continuous rather than categorical variables). The estimator to use for this is the ``RandomForestRegressor``, and the syntax is very similar to what we saw earlier.\n",
    "\n",
    "Consider the following data, drawn from the combination of a fast and slow oscillation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1663246465858,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "TuDNARVZObve",
    "outputId": "021d7a77-9694-49d9-e9ba-5e70ff028e54"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(200)\n",
    "\n",
    "\n",
    "def model(x, sigma=0.3):\n",
    "    fast_oscillation = np.sin(5 * x)\n",
    "    slow_oscillation = np.sin(0.5 * x)\n",
    "    noise = sigma * rng.randn(len(x))\n",
    "\n",
    "    return slow_oscillation + fast_oscillation + noise\n",
    "\n",
    "\n",
    "y = model(x)\n",
    "plt.errorbar(x, y, 0.3, fmt=\"o\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DVeDTAaObvf"
   },
   "source": [
    "Using the random forest regressor, we can find the best fit curve as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "executionInfo": {
     "elapsed": 1187,
     "status": "ok",
     "timestamp": 1663246470674,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "XgPwf4sQObvf",
    "outputId": "90029337-0f3b-4bc9-c421-631eee60d56d"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(200)\n",
    "forest.fit(x[:, None], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = forest.predict(xfit[:, None])\n",
    "ytrue = model(xfit, sigma=0)\n",
    "\n",
    "plt.errorbar(x, y, 0.3, fmt=\"o\", alpha=0.5)\n",
    "plt.plot(xfit, yfit, \"-r\")\n",
    "plt.plot(xfit, ytrue, \"-k\", alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5vuFbf9Obvf"
   },
   "source": [
    "Here the true model is shown in the smooth gray curve, while the random forest model is shown by the jagged red curve.\n",
    "As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us needing to specifying a multi-period model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XI616_MqObvf"
   },
   "source": [
    "## Example: Predicting diamonds price\n",
    "\n",
    "We use again the example we explored for linear models: we will try and predict the price of diamonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1663141390150,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "3TsQDgISObvf",
    "outputId": "d46e37ef-c88a-41c4-d901-c6a6a6e40204"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "diamonds_raw = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/tidyverse/ggplot2/master/data-raw/diamonds.csv\"\n",
    ")\n",
    "diamonds = diamonds_raw.sample(5000, random_state=0)\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP7irjaKbbpC"
   },
   "source": [
    "As before, we remove the data with zero dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vURnsqBvTO5S"
   },
   "outputs": [],
   "source": [
    "diamonds = diamonds[diamonds.x * diamonds.y * diamonds.z != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Mbh-JxUbhRR"
   },
   "source": [
    "And we exploit the results of the previous exploratory analysis to understand which variables to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1663141390151,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "0RRXOuEVTO3R",
    "outputId": "d18c043c-15e4-40de-a9ff-9a5932061a92"
   },
   "outputs": [],
   "source": [
    "diamonds_processed = diamonds.drop(columns=[\"depth\", \"table\", \"y\", \"z\"])\n",
    "diamonds_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZYNlwAEbsSC"
   },
   "source": [
    "Then we create the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bgda6nX3YyAM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = diamonds_processed.drop(columns=\"price\")\n",
    "y = diamonds_processed.price\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r70HLWkZbw6B"
   },
   "source": [
    "And finally we create the pipeline for the model. We use the ordinal encoder as all the categorical variables have a natural ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jes6Kon1TO0w"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "column_transformer = make_column_transformer(\n",
    "    (\n",
    "        OrdinalEncoder(categories=[[\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"]]),\n",
    "        [\"cut\"],\n",
    "    ),\n",
    "    (\n",
    "        OrdinalEncoder(\n",
    "            categories=[[\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"]]\n",
    "        ),\n",
    "        [\"clarity\"],\n",
    "    ),\n",
    "    (OrdinalEncoder(categories=[[\"J\", \"I\", \"H\", \"G\", \"F\", \"E\", \"D\"]]), [\"color\"]),\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "rf_model = RandomForestRegressor(n_estimators=1000, max_depth=5, random_state=42)\n",
    "\n",
    "rf_pipeline = make_pipeline(column_transformer, rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTSFwopfb8-6"
   },
   "source": [
    "We fit the model to the train set and we assess the metrics on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4906,
     "status": "ok",
     "timestamp": 1663141395052,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "ZG7Q6K1-TOyS",
    "outputId": "f3847357-a7eb-422f-eb70-acc71914849c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "rf_pipeline.fit(x_train, y_train)\n",
    "pred = rf_pipeline.predict(x_test)\n",
    "\n",
    "print(f\"R2 Score: {round(r2_score(y_test, pred), 4)}\")\n",
    "print(f\"MAE: {round(mean_absolute_error(y_test, pred), 2)}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "executionInfo": {
     "elapsed": 663,
     "status": "ok",
     "timestamp": 1663141395712,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "GOhxhbQ1TOvh",
    "outputId": "5128007d-ac52-4252-8cd3-a5ec3d9adc9c"
   },
   "outputs": [],
   "source": [
    "def plot_gof(y_true, y_pred):\n",
    "    plt.plot(y_test, pred, \".\")\n",
    "    plt.plot(y_test, y_test, linewidth=3, c=\"black\")\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_gof(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82yLuTOdcKV0"
   },
   "source": [
    "We see that there are no predictions below zero and that the cheaper diamods have better predictions. Maybe a Gradient Boosting model could do better?\n",
    "We will not try and assess it, but we will try to improve out Random Forest. \n",
    "\n",
    "For instance, by trying and optimizing the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGMZxukXTOpY"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_model_cv = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "cv_grid = {\n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [5, 7, 9],\n",
    "    \"criterion\": [\"squared_error\"],\n",
    "    \"min_samples_split\": [2, 4, 8],\n",
    "}\n",
    "\n",
    "rf_pipeline = make_pipeline(\n",
    "    column_transformer, GridSearchCV(rf_model_cv, cv_grid, cv=5, verbose=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54832,
     "status": "ok",
     "timestamp": 1663141450536,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "jTvLEUW8crOJ",
    "outputId": "7c6f1cb3-7299-42fb-dd5f-ee1f29948aa4"
   },
   "outputs": [],
   "source": [
    "rf_pipeline.fit(x_train, y_train)\n",
    "pred = rf_pipeline.predict(x_test)\n",
    "\n",
    "print(f\"R2 Score: {round(r2_score(y_test, pred), 4)}\")\n",
    "print(f\"MAE: {round(mean_absolute_error(y_test, pred), 2)}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1663141450537,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "4-5ppOIrcrL5",
    "outputId": "9b8f2c65-1629-4f1b-d8dd-f60e3711644a"
   },
   "outputs": [],
   "source": [
    "def plot_gof(y_true, y_pred):\n",
    "    plt.plot(y_test, pred, \".\")\n",
    "    plt.plot(y_test, y_test, linewidth=3, c=\"black\")\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_gof(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK4wT8JchY40"
   },
   "source": [
    "We have improved a lot! Let's check out the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1663141450538,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "9UiL8O67TOh5",
    "outputId": "70694ed6-63f4-4aef-e6eb-ba788435cb01"
   },
   "outputs": [],
   "source": [
    "rf_pipeline[1].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlpbCuTvhjB0"
   },
   "source": [
    "Now, we hit the limits of the grid for both `max_depth` and `n_estimators`. Possibly, a different value for such hyperparameters could have resulted in better performance. \n",
    "\n",
    "Finally, let's have a look to the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1663141451041,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "19zYtCvXhigc",
    "outputId": "d81e7ff8-e87c-482a-8800-a16bb66c9719"
   },
   "outputs": [],
   "source": [
    "forest_importances = pd.Series(\n",
    "    rf_pipeline[1].best_estimator_.feature_importances_, index=x_train.columns\n",
    ")\n",
    "forest_importances.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "executionInfo": {
     "elapsed": 10833,
     "status": "ok",
     "timestamp": 1663141461870,
     "user": {
      "displayName": "Emanuele Fabbiani",
      "userId": "14148944018343998200"
     },
     "user_tz": -120
    },
    "id": "Z152WQD-i6UM",
    "outputId": "8878ed0f-f0c3-4c5c-94b5-c3037ec060b3"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    rf_pipeline, x_test, y_test, n_repeats=20, random_state=42\n",
    ")\n",
    "pd.Series(perm_importance[\"importances_mean\"], index=x_train.columns).plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_zJYoVOjjgg"
   },
   "source": [
    "We see that the two metrics do not coincide. From what we know about the dataset, we are more incline to believe to the permutation importance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.08-Random-Forests.ipynb",
     "timestamp": 1662911105251
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
